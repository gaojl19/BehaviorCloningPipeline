/var/lib/slurm/slurmd/job3515631/slurm_script: line 9: batch_scripts/MT50_batch/seed.sh: No such file or directory
SLURM_JOBID=3515631
working directory=/viscam/u/jialugao/Imitation-SoftModule
########################
logging outputs to  /viscam/u/jialugao/Imitation-SoftModule/data/Base_bc_reach_push-back-v1_2640.000125-01-2022_23-01-55
########################
{'expert_policy_file': '../Multi-Task-RL/log/MT50_Single_Task/push-back-v1/Fixed/238/model/model_pf_best.pth', 'exp_name': 'bc_reach', 'do_dagger': False, 'ep_len': 200, 'gradient_steps': 1, 'n_iter': 10000, 'render_interval': 1, 'eval_interval': 200, 'batch_size': 64, 'eval_batch_size': 32, 'train_batch_size': 32, 'n_layers': 2, 'size': 400, 'learning_rate': 0.0001, 'video_log_freq': -1, 'scalar_log_freq': 1, 'no_gpu': False, 'which_gpu': 0, 'max_replay_buffer_size': 1000000, 'save_params': False, 'seed': 32, 'worker_nums': 1, 'eval_worker_nums': 1, 'config': 'config/BC.json', 'no_cuda': True, 'random_init': False, 'device': 'cpu', 'id': 'MT50_Single_Task', 'task_name': 'push-back-v1', 'task_env': 'MT50_task_env', 'cuda': False, 'log_dir': '/viscam/u/jialugao/Imitation-SoftModule/data/Base_bc_reach_push-back-v1_2640.000125-01-2022_23-01-55', 'agent_class': <class 'agents.bc_agent.MLPAgent'>, 'agent_params': {'n_layers': 2, 'size': 400, 'learning_rate': 0.0001, 'max_replay_buffer_size': 1000000, 'discrete': False, 'ac_dim': 4, 'ob_dim': 9}}
{'env_name': 'single_task', 'env': {'reward_scale': 1, 'obs_norm': False}, 'meta_env': {'obs_type': 'with_goal', 'random_init': False}, 'replay_buffer': {'size': 1000000.0}, 'net': {'hidden_shapes': [400, 400], 'append_hidden_shapes': [400], 'base_type': <class 'networks.base.MLPBase'>}, 'general_setting': {'discount': 0.99, 'pretrain_epochs': 20, 'num_epochs': 7500, 'epoch_frames': 200, 'max_episode_frames': 200, 'batch_size': 1280, 'min_pool': 10000, 'target_hard_update_period': 1000, 'use_soft_update': True, 'tau': 0.005, 'opt_times': 200, 'eval_episodes': 100, 'train_render': False, 'eval_render': False, 'env': <NormAct<RewardShift<SingleWrapperNone>>>, 'logger': <utils.logger.Logger object at 0x7f12651dfe50>, 'device': device(type='cpu')}, 'sac': {'plr': 0.0003, 'qlr': 0.0003, 'reparameterization': True, 'automatic_entropy_tuning': True, 'policy_std_reg_weight': 0, 'policy_mean_reg_weight': 0}}
Loading expert policy from... ../Multi-Task-RL/log/MT50_Single_Task/push-back-v1/Fixed/238/model/model_pf_best.pth
tensor([[-0.0419, -0.0292, -0.1226,  ..., -0.0365, -0.0079, -0.0537],
        [ 0.0337, -0.0368, -0.0461,  ...,  0.0090, -0.0083, -0.0774],
        [-0.0928, -0.1028, -0.2461,  ..., -0.0016,  0.0063, -0.0153],
        ...,
        [ 0.0816, -0.0822, -0.0029,  ...,  0.0400, -0.0482, -0.0429],
        [-0.0300, -0.0367, -0.0532,  ..., -0.0461,  0.0085, -0.0165],
        [-0.0481,  0.0077, -0.2784,  ..., -0.0211, -0.0420, -0.0097]])
Done restoring expert policy...
<NormAct<RewardShift<AugObs<SawyerPushBackEnv instance>>>>


-------------------------------- Iteration 0 -------------------------------- 
initial ob: [-0.03265248  0.51488137  0.23688703  0.          0.8         0.01999588
  0.          0.6         0.02        0.          0.6         0.02      ]
expert:[0.96730584 0.9771383  0.989942   0.91134137]
expert:[0.9577844  0.9708532  0.9905578  0.89513266]
expert:[0.94215995 0.9658405  0.9907609  0.8821684 ]
expert:[0.89148974 0.9640337  0.9897602  0.8731912 ]
expert:[-0.37861905  0.9718568   0.97699064  0.87450886]
expert:[-0.9218272  0.9749244  0.8860958  0.8339608]
expert:[-0.9305993   0.9414654  -0.3802937   0.76473373]
expert:[-0.8880655   0.905076   -0.8092181   0.75964886]
expert:[-0.8239051   0.8728608  -0.83981276  0.73396975]
expert:[-0.65879595  0.8136885  -0.8164127   0.66095567]
expert:[ 0.0924902   0.7014577  -0.7644407   0.42501143]
expert:[ 0.64470744  0.61960626 -0.7481765   0.30370125]
expert:[ 0.65107834  0.6006382  -0.36977786  0.32436696]
expert:[0.60216075 0.62791747 0.62144953 0.45616212]
expert:[0.48936254 0.52249616 0.77439916 0.46985587]
expert:[0.24836192 0.30261117 0.7537666  0.4287381 ]
expert:[0.02373986 0.05685165 0.6974289  0.43371046]
expert:[-0.08220532 -0.06326227  0.5852712   0.43664405]
expert:[-0.0969752  -0.03486144  0.42128617  0.40348297]
expert:[-0.10002499  0.06265966  0.19832797  0.3746901 ]
expert:[-0.0938146   0.15580511 -0.02516597  0.3603913 ]
expert:[-0.07479245  0.22831455 -0.20481251  0.3487512 ]
expert:[-0.04075588  0.26987207 -0.30500004  0.33461878]
expert:[ 0.00103876  0.2736937  -0.3342494   0.32025617]
expert:[ 0.03875048  0.2410345  -0.29674914  0.30970633]
expert:[ 0.05756567  0.18068539 -0.20700483  0.30916184]
expert:[ 0.05739532  0.10427126 -0.08394415  0.31637895]
expert:[0.03357209 0.0255707  0.03923718 0.33283493]
expert:[ 0.00615285 -0.03944769  0.14163886  0.34816995]
expert:[-0.01829431 -0.08556011  0.2071684   0.36195272]
expert:[-0.03087741 -0.10370789  0.22861588  0.3679209 ]
expert:[-0.02841575 -0.09409248  0.20791417  0.36383197]
expert:[-0.01401415 -0.06241084  0.15290426  0.35285157]
expert:[ 0.00483829 -0.01876875  0.07780412  0.34052524]
expert:[ 0.0222408   0.02631755 -0.00195842  0.32985482]
expert:[ 0.03369312  0.0651244  -0.0710689   0.32203612]
expert:[ 0.03722083  0.0900017  -0.1147879   0.31814784]
expert:[ 0.03311298  0.09686346 -0.12780203  0.3182011 ]
expert:[ 0.02304194  0.08734264 -0.11314768  0.3215998 ]
expert:[ 0.01004421  0.06646973 -0.07899432  0.3270185 ]
expert:[-0.00406364  0.03841116 -0.03317176  0.33350423]
expert:[-0.01594872  0.00950248  0.01349856  0.33941367]
expert:[-0.02401355 -0.01463745  0.05237276  0.34395057]
expert:[-0.02706311 -0.0303653   0.07701124  0.3466098 ]
expert:[-0.02486904 -0.03618493  0.08435992  0.34666133]
expert:[-0.01796964 -0.03210268  0.07501285  0.34423453]
expert:[-0.00799052 -0.02014238  0.05297581  0.340054  ]
expert:[ 0.00275459 -0.00368425  0.0243852   0.33553562]
expert:[ 0.01270577  0.01313031 -0.00513643  0.33111474]
expert:[ 0.01953989  0.02689606 -0.02941434  0.3277944 ]
expert:[ 0.02214825  0.03507619 -0.04403453  0.32616192]
expert:[ 0.02043731  0.03642749 -0.04670305  0.326376  ]
expert:[ 0.01499388  0.03189803 -0.03927558  0.3281696 ]
expert:[ 0.0072017   0.02332346 -0.02513395  0.33099228]
expert:[-0.00102568  0.01270983 -0.00766973  0.3341247 ]
expert:[-0.00782464  0.00212531  0.00920132  0.33685014]
expert:[-0.01234156 -0.00656351  0.02297094  0.33880442]
expert:[-0.01431928 -0.01261001  0.03236961  0.33987972]
expert:[-0.01350558 -0.01452973  0.03480878  0.33984098]
expert:[-0.01011425 -0.01172404  0.02927826  0.33866736]
expert:[-0.0050842  -0.00559665  0.0182392   0.33674273]
expert:[0.00057451 0.00195046 0.00502813 0.33449233]
expert:[ 0.0057303   0.00891481 -0.00748405  0.33240807]
expert:[ 0.00932963  0.01376345 -0.01617465  0.33094442]
expert:[ 0.01074014  0.0152683  -0.01899049  0.3304092 ]
expert:[ 0.00994045  0.01367996 -0.01640778  0.33077484]
expert:[ 0.00733273  0.01014222 -0.01049099  0.33180037]
expert:[ 0.00365711  0.00574104 -0.00309668  0.33317497]
expert:[-2.0027161e-04  1.3087981e-03  4.2891055e-03  3.3458304e-01]
expert:[-0.00349689 -0.00249086  0.0103161   0.3357675 ]
expert:[-0.00565797 -0.00476691  0.01378901  0.33649313]
expert:[-0.00644442 -0.00497654  0.01373185  0.33665904]
expert:[-0.00583504 -0.00320227  0.01031255  0.33627456]
expert:[-4.1369409e-03 -2.1106005e-04  4.8699924e-03  3.3550248e-01]
expert:[-0.00180787  0.00300904 -0.00086158  0.33455604]
expert:[ 0.00058704  0.00545461 -0.0053036   0.3336804 ]
expert:[ 0.00252266  0.00648547 -0.00737889  0.33307895]
expert:[ 0.00365337  0.00607866 -0.00690867  0.33284202]
expert:[ 0.00389212  0.00461136 -0.00455876  0.33294415]
expert:[ 0.00334133  0.00262499 -0.00128009  0.33329532]
expert:[0.00224028 0.00065545 0.00198872 0.33377466]
expert:[ 0.00090183 -0.00084447  0.00445449  0.33425415]
expert:[-0.00034173 -0.0015922   0.00554791  0.3346189 ]
expert:[-0.00125502 -0.00146979  0.00514171  0.3348028 ]
expert:[-0.00170783 -0.00063393  0.00354308  0.3347948 ]
expert:[-0.0017091   0.00056099  0.00136431  0.33464122]
expert:[-0.00135955  0.00170772 -0.00070352  0.33441344]
expert:[-0.00081351  0.00247491 -0.00210616  0.33418578]
expert:[-2.1662563e-04  2.6962021e-03 -2.6018971e-03  3.3400872e-01]
expert:[ 2.9268861e-04  2.4080081e-03 -2.2386536e-03  3.3391184e-01]
expert:[ 0.000631    0.00177081 -0.00127016  0.33389664]
expert:[ 7.7463669e-04  9.9476392e-04 -5.6628138e-05  3.3394390e-01]
expert:[7.4642885e-04 2.8927624e-04 1.0427121e-03 3.3402348e-01]
expert:[ 5.9916073e-04 -1.8447638e-04  1.7540549e-03  3.3410451e-01]
expert:[ 0.00039816 -0.00034975  0.00194466  0.33416247]
expert:[ 1.9976497e-04 -2.1868199e-04  1.6378857e-03  3.3418590e-01]
expert:[4.1916966e-05 1.1240691e-04 9.9897722e-04 3.3417729e-01]
expert:[-6.0819089e-05  5.0742173e-04  2.6038662e-04  3.3414918e-01]
expert:[-1.1335313e-04  8.3730352e-04 -3.5754216e-04  3.3411783e-01]
expert:[-1.2907386e-04  1.0143515e-03 -7.0931378e-04  3.3409625e-01]
expert:[-1.2423843e-04  1.0122877e-03 -7.5278047e-04  3.3409116e-01]
expert:[-1.1051446e-04  8.6060143e-04 -5.4096797e-04  3.3410138e-01]
expert:[-9.3258917e-05  6.1994785e-04 -1.8035993e-04  3.3412096e-01]
expert:[-7.2397292e-05  3.6487725e-04  2.0097569e-04  3.3414105e-01]
expert:[-4.5068562e-05  1.6307831e-04  4.8975268e-04  3.3415321e-01]
expert:[-8.9555979e-06  5.1788986e-05  6.2251458e-04  3.3415270e-01]
expert:[3.4525990e-05 4.1358173e-05 5.8570126e-04 3.3413875e-01]
expert:[8.0212951e-05 1.0842085e-04 4.2092425e-04 3.3411568e-01]
expert:[1.19931996e-04 2.15448439e-04 1.94344670e-04 3.34089845e-01]
expert:[ 1.4486164e-04  3.1824410e-04 -1.8130988e-05  3.3406866e-01]
expert:[ 1.4842302e-04  3.8518754e-04 -1.6199425e-04  3.3405733e-01]
expert:[ 1.2901425e-04  3.9772686e-04 -2.0806864e-04  3.3405814e-01]
expert:[ 9.0025365e-05  3.5822389e-04 -1.6195700e-04  3.3406991e-01]
expert:[ 3.9391220e-05  2.8513372e-04 -5.6464225e-05  3.3408856e-01]
expert:[-1.2114644e-05  1.9997358e-04  6.9085509e-05  3.3410874e-01]
expert:[-5.4121017e-05  1.2799352e-04  1.7093495e-04  3.3412507e-01]
expert:[-7.8558922e-05  8.3141029e-05  2.2504851e-04  3.3413398e-01]
expert:[-8.1978738e-05  7.0370734e-05  2.2259727e-04  3.3413404e-01]
expert:[-6.6161156e-05  8.7320805e-05  1.7009303e-04  3.3412623e-01]
expert:[-3.6112964e-05  1.1963397e-04  9.1601163e-05  3.3411330e-01]
expert:[-4.2468309e-07  1.5407801e-04  1.1939555e-05  3.3409905e-01]
expert:[ 3.2991171e-05  1.7617643e-04 -4.5221299e-05  3.3408692e-01]
expert:[ 5.7190657e-05  1.8021464e-04 -6.9033355e-05  3.3407924e-01]
expert:[ 6.8247318e-05  1.6599894e-04 -5.8840960e-05  3.3407706e-01]
expert:[ 6.6049397e-05  1.3679266e-04 -2.2288412e-05  3.3407980e-01]
expert:[5.2660704e-05 1.0249764e-04 2.4963170e-05 3.3408591e-01]
expert:[3.2894313e-05 7.2062016e-05 6.6537410e-05 3.3409312e-01]
expert:[1.16452575e-05 5.07682562e-05 9.26442444e-05 3.34099680e-01]
expert:[-6.4522028e-06  4.3027103e-05  9.6034259e-05  3.3410388e-01]
expert:[-1.8507242e-05  4.6141446e-05  8.1177801e-05  3.3410546e-01]
expert:[-2.3193657e-05  5.7332218e-05  5.2552670e-05  3.3410439e-01]
expert:[-2.1040440e-05  6.9953501e-05  2.1491200e-05  3.3410153e-01]
expert:[-1.4387071e-05  8.0719590e-05 -5.3830445e-06  3.3409792e-01]
expert:[-5.4165721e-06  8.4333122e-05 -1.9770116e-05  3.3409470e-01]
expert:[ 3.3080578e-06  8.0890954e-05 -2.1632761e-05  3.3409238e-01]
expert:[ 1.0468066e-05  7.0750713e-05 -1.2066215e-05  3.3409119e-01]
expert:[1.4543533e-05 5.7376921e-05 4.0419400e-06 3.3409122e-01]
expert:[1.5579164e-05 4.3921173e-05 2.0582229e-05 3.3409190e-01]
expert:[1.44690275e-05 3.29166651e-05 3.29576433e-05 3.34092915e-01]
expert:[1.1608005e-05 2.7388334e-05 3.7726015e-05 3.3409381e-01]
expert:[8.1956387e-06 2.5629997e-05 3.5535544e-05 3.3409429e-01]
expert:[5.1185489e-06 2.7708709e-05 2.7004629e-05 3.3409435e-01]
expert:[2.5257468e-06 3.1217933e-05 1.6774982e-05 3.3409414e-01]
expert:[8.7916851e-07 3.4861267e-05 6.3516200e-06 3.3409372e-01]
expert:[-1.7136335e-07  3.6515296e-05 -5.1781535e-07  3.3409342e-01]
expert:[-7.0780516e-07  3.6008656e-05 -3.0063093e-06  3.3409327e-01]
expert:[-7.4505806e-07  3.3751130e-05 -2.4326146e-06  3.3409327e-01]
expert:[-6.7055225e-07  2.9303133e-05  2.0302832e-06  3.3409348e-01]
expert:[-2.7567148e-07  2.4758279e-05  6.5825880e-06  3.3409366e-01]
expert:[7.4505806e-09 2.0690262e-05 1.0814518e-05 3.3409384e-01]
expert_success: 0
path_length: 150



-------------------------------- Iteration 0 -------------------------------- 
initial ob: [-0.03265248  0.51488137  0.23688703  0.          0.8         0.01999588
  0.          0.6         0.02        0.          0.6         0.02      ]
agent_success: 0

training time:  0.136796236038208
evaluation time:  7.463482141494751
epoch time:  7.600308895111084


-------------------------------- Iteration 200 -------------------------------- 
initial ob: [-0.03265248  0.51488137  0.23688703  0.          0.8         0.01999588
  0.          0.6         0.02        0.          0.6         0.02      ]
agent_success: 0

training time:  0.0023229122161865234
evaluation time:  7.448710918426514
epoch time:  7.451064586639404


-------------------------------- Training stopped due to early stopping -------------------------------- 
min loss:  0.0035537432


-------------------------------- Test Results -------------------------------- 
initial ob: [-0.03265248  0.51488137  0.23688703  0.          0.8         0.01999588
  0.          0.6         0.02        0.          0.6         0.02      ]
agent_success: 0

mean_success_rate:  0.0
Done
