/var/lib/slurm/slurmd/job3511550/slurm_script: line 9: batch_scripts/MT50_batch/seed.sh: No such file or directory
SLURM_JOBID=3511550
working directory=/viscam/u/jialugao/Imitation-SoftModule
########################
logging outputs to  /viscam/u/jialugao/Imitation-SoftModule/data/Base_bc_reach_handle-pull-v1_2640.000123-01-2022_22-52-07
########################
{'expert_policy_file': '../Multi-Task-RL/log/MT50_Single_Task/handle-pull-v1/Fixed/238/model/model_pf_best.pth', 'exp_name': 'bc_reach', 'do_dagger': False, 'ep_len': 200, 'gradient_steps': 1, 'n_iter': 10000, 'render_interval': 1, 'eval_interval': 200, 'batch_size': 64, 'eval_batch_size': 32, 'train_batch_size': 32, 'n_layers': 2, 'size': 400, 'learning_rate': 0.0001, 'video_log_freq': -1, 'scalar_log_freq': 1, 'no_gpu': False, 'which_gpu': 0, 'max_replay_buffer_size': 1000000, 'save_params': False, 'seed': 32, 'worker_nums': 1, 'eval_worker_nums': 1, 'config': 'config/BC.json', 'no_cuda': True, 'random_init': False, 'device': 'cpu', 'id': 'MT50_Single_Task', 'task_name': 'handle-pull-v1', 'task_env': 'MT50_task_env', 'cuda': False, 'log_dir': '/viscam/u/jialugao/Imitation-SoftModule/data/Base_bc_reach_handle-pull-v1_2640.000123-01-2022_22-52-07', 'agent_class': <class 'agents.bc_agent.MLPAgent'>, 'agent_params': {'n_layers': 2, 'size': 400, 'learning_rate': 0.0001, 'max_replay_buffer_size': 1000000, 'discrete': False, 'ac_dim': 4, 'ob_dim': 9}}
{'env_name': 'single_task', 'env': {'reward_scale': 1, 'obs_norm': False}, 'meta_env': {'obs_type': 'with_goal', 'random_init': False}, 'replay_buffer': {'size': 1000000.0}, 'net': {'hidden_shapes': [400, 400], 'append_hidden_shapes': [400], 'base_type': <class 'networks.base.MLPBase'>}, 'general_setting': {'discount': 0.99, 'pretrain_epochs': 20, 'num_epochs': 7500, 'epoch_frames': 200, 'max_episode_frames': 200, 'batch_size': 1280, 'min_pool': 10000, 'target_hard_update_period': 1000, 'use_soft_update': True, 'tau': 0.005, 'opt_times': 200, 'eval_episodes': 100, 'train_render': False, 'eval_render': False, 'env': <NormAct<RewardShift<SingleWrapperNone>>>, 'logger': <utils.logger.Logger object at 0x7f7a9890acd0>, 'device': device(type='cpu')}, 'sac': {'plr': 0.0003, 'qlr': 0.0003, 'reparameterization': True, 'automatic_entropy_tuning': True, 'policy_std_reg_weight': 0, 'policy_mean_reg_weight': 0}}
Loading expert policy from... ../Multi-Task-RL/log/MT50_Single_Task/handle-pull-v1/Fixed/238/model/model_pf_best.pth
Done restoring expert policy...
<NormAct<RewardShift<SawyerHandlePullEnv instance>>>
<NormAct<RewardShift<SawyerHandlePullEnv instance>>>


-------------------------------- Iteration 0 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
expert_success: 1.0



-------------------------------- Iteration 0 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.24749517440795898
evaluation time:  6.664132118225098
epoch time:  6.911659240722656


-------------------------------- Iteration 200 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.002309083938598633
evaluation time:  7.56274151802063
epoch time:  7.565078496932983


-------------------------------- Iteration 400 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.002382993698120117
evaluation time:  7.551375389099121
epoch time:  7.553784132003784


-------------------------------- Iteration 600 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023543834686279297
evaluation time:  7.567928791046143
epoch time:  7.570314645767212


-------------------------------- Iteration 800 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023877620697021484
evaluation time:  7.462430715560913
epoch time:  7.464848041534424


-------------------------------- Iteration 1000 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023713111877441406
evaluation time:  7.4595489501953125
epoch time:  7.461944341659546


-------------------------------- Iteration 1200 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023317337036132812
evaluation time:  7.388730764389038
epoch time:  7.391087055206299


-------------------------------- Iteration 1400 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023069381713867188
evaluation time:  7.417271614074707
epoch time:  7.419605493545532


-------------------------------- Iteration 1600 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0022902488708496094
evaluation time:  7.304861068725586
epoch time:  7.307173252105713


-------------------------------- Iteration 1800 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023033618927001953
evaluation time:  7.400006532669067
epoch time:  7.402329206466675


-------------------------------- Iteration 2000 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.002301931381225586
evaluation time:  7.303237199783325
epoch time:  7.305605888366699


-------------------------------- Iteration 2200 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0022928714752197266
evaluation time:  7.296976566314697
epoch time:  7.299296855926514


-------------------------------- Iteration 2400 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023005008697509766
evaluation time:  7.298341989517212
epoch time:  7.300666332244873


-------------------------------- Iteration 2600 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.002299070358276367
evaluation time:  7.361742258071899
epoch time:  7.36405873298645


-------------------------------- Iteration 2800 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.002302885055541992
evaluation time:  7.314201593399048
epoch time:  7.3165318965911865


-------------------------------- Iteration 3000 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0022890567779541016
evaluation time:  7.317408084869385
epoch time:  7.319711923599243


-------------------------------- Iteration 3200 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023016929626464844
evaluation time:  7.393542528152466
epoch time:  7.395874500274658


-------------------------------- Iteration 3400 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023043155670166016
evaluation time:  7.453918933868408
epoch time:  7.456250190734863


-------------------------------- Iteration 3600 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023114681243896484
evaluation time:  7.3859357833862305
epoch time:  7.3882622718811035


-------------------------------- Iteration 3800 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0023195743560791016
evaluation time:  7.5101728439331055
epoch time:  7.512526750564575


-------------------------------- Iteration 4000 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0024886131286621094
evaluation time:  7.420827865600586
epoch time:  7.4233317375183105


-------------------------------- Iteration 4200 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0022983551025390625
evaluation time:  7.478039264678955
epoch time:  7.4803571701049805


-------------------------------- Iteration 4400 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.0022916793823242188
evaluation time:  7.492149591445923
epoch time:  7.494452476501465


-------------------------------- Iteration 4600 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.002315521240234375
evaluation time:  7.486916780471802
epoch time:  7.489251613616943


-------------------------------- Iteration 4800 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.002314329147338867
evaluation time:  7.454138278961182
epoch time:  7.456466913223267


-------------------------------- Iteration 5000 -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

training time:  0.002309083938598633
evaluation time:  7.485486745834351
epoch time:  7.487809896469116


-------------------------------- Training stopped due to early stopping -------------------------------- 
min loss:  0.09893217


-------------------------------- Test Results -------------------------------- 
initial ob: [-0.03265156  0.51487877  0.23688007  0.          0.75        0.04
  0.          0.75        0.16      ]
agent_success: 0

mean_success_rate:  0.0
Done
