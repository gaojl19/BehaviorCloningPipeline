/var/lib/slurm/slurmd/job3513850/slurm_script: line 9: batch_scripts/MT50_batch/seed.sh: No such file or directory
SLURM_JOBID=3513850
working directory=/viscam/u/jialugao/Imitation-SoftModule
########################
logging outputs to  /viscam/u/jialugao/Imitation-SoftModule/data/Base_bc_reach_hammer-v1_2640.000124-01-2022_18-56-06
########################
{'expert_policy_file': '../Multi-Task-RL/log/MT50_Single_Task/hammer-v1/Fixed/238/model/model_pf_best.pth', 'exp_name': 'bc_reach', 'do_dagger': False, 'ep_len': 200, 'gradient_steps': 1, 'n_iter': 10000, 'render_interval': 1, 'eval_interval': 200, 'batch_size': 64, 'eval_batch_size': 32, 'train_batch_size': 32, 'n_layers': 2, 'size': 400, 'learning_rate': 0.0001, 'video_log_freq': -1, 'scalar_log_freq': 1, 'no_gpu': False, 'which_gpu': 0, 'max_replay_buffer_size': 1000000, 'save_params': False, 'seed': 32, 'worker_nums': 1, 'eval_worker_nums': 1, 'config': 'config/BC.json', 'no_cuda': True, 'random_init': False, 'device': 'cpu', 'id': 'MT50_Single_Task', 'task_name': 'hammer-v1', 'task_env': 'MT50_task_env', 'cuda': False, 'log_dir': '/viscam/u/jialugao/Imitation-SoftModule/data/Base_bc_reach_hammer-v1_2640.000124-01-2022_18-56-06', 'agent_class': <class 'agents.bc_agent.MLPAgent'>, 'agent_params': {'n_layers': 2, 'size': 400, 'learning_rate': 0.0001, 'max_replay_buffer_size': 1000000, 'discrete': False, 'ac_dim': 4, 'ob_dim': 9}}
{'env_name': 'single_task', 'env': {'reward_scale': 1, 'obs_norm': False}, 'meta_env': {'obs_type': 'with_goal', 'random_init': False}, 'replay_buffer': {'size': 1000000.0}, 'net': {'hidden_shapes': [400, 400], 'append_hidden_shapes': [400], 'base_type': <class 'networks.base.MLPBase'>}, 'general_setting': {'discount': 0.99, 'pretrain_epochs': 20, 'num_epochs': 7500, 'epoch_frames': 200, 'max_episode_frames': 200, 'batch_size': 1280, 'min_pool': 10000, 'target_hard_update_period': 1000, 'use_soft_update': True, 'tau': 0.005, 'opt_times': 200, 'eval_episodes': 100, 'train_render': False, 'eval_render': False, 'env': <NormAct<RewardShift<SingleWrapperNone>>>, 'logger': <utils.logger.Logger object at 0x7f176cc8be10>, 'device': device(type='cpu')}, 'sac': {'plr': 0.0003, 'qlr': 0.0003, 'reparameterization': True, 'automatic_entropy_tuning': True, 'policy_std_reg_weight': 0, 'policy_mean_reg_weight': 0}}
Loading expert policy from... ../Multi-Task-RL/log/MT50_Single_Task/hammer-v1/Fixed/238/model/model_pf_best.pth
tensor([[ 0.0264,  0.0149, -0.0274,  ..., -0.0425,  0.0107, -0.0352],
        [-0.0049, -0.0261, -0.0296,  ..., -0.0205,  0.0018, -0.0679],
        [-0.0230, -0.0592, -0.0410,  ..., -0.0168, -0.0037, -0.0256],
        ...,
        [ 0.0805, -0.0689,  0.0054,  ...,  0.0118, -0.0373, -0.0325],
        [-0.0081, -0.0217, -0.0226,  ..., -0.0674,  0.0228, -0.0027],
        [ 0.0249,  0.0065, -0.0418,  ..., -0.0475, -0.0403, -0.0084]])
Done restoring expert policy...
<NormAct<RewardShift<AugObs<SawyerHammerEnv instance>>>>


-------------------------------- Iteration 0 -------------------------------- 
initial ob: [-0.03265025  0.5148823   0.23687374  0.          0.6         0.02
  0.24        0.74        0.11        0.24        0.74        0.11      ]
expert:[ 0.00687689 -0.00759514 -0.06156104  0.01719864]
expert:[ 0.00687566 -0.00759647 -0.06156153  0.01720014]
expert:[ 0.00687377 -0.00759839 -0.06156517  0.01720362]
expert:[ 0.00687226 -0.00760004 -0.06156913  0.01720677]
expert:[ 0.00687116 -0.0076012  -0.06157232  0.01720913]
expert:[ 0.00687039 -0.00760195 -0.06157462  0.01721081]
expert:[ 0.00686987 -0.00760247 -0.06157632  0.017212  ]
expert:[ 0.00686951 -0.00760285 -0.06157764  0.01721288]
expert:[ 0.00686925 -0.00760312 -0.06157866  0.01721357]
expert:[ 0.00686905 -0.00760334 -0.06157953  0.01721412]
expert:[ 0.00686888 -0.00760351 -0.06158029  0.01721461]
expert:[ 0.00686873 -0.00760367 -0.06158098  0.01721504]
expert:[ 0.0068686  -0.00760381 -0.06158163  0.01721545]
expert:[ 0.00686847 -0.00760395 -0.06158226  0.01721585]
expert:[ 0.00686834 -0.00760408 -0.06158288  0.01721623]
expert:[ 0.00686822 -0.00760421 -0.06158349  0.01721661]
expert:[ 0.0068681  -0.00760434 -0.0615841   0.01721699]
expert:[ 0.00686798 -0.00760446 -0.0615847   0.01721737]
expert:[ 0.00686785 -0.00760459 -0.0615853   0.01721775]
expert:[ 0.00686773 -0.00760472 -0.06158591  0.01721812]
expert:[ 0.00686763 -0.00760483 -0.06158648  0.01721846]
expert:[ 0.00686751 -0.00760495 -0.06158705  0.01721882]
expert:[ 0.00686739 -0.00760508 -0.06158766  0.0172192 ]
expert:[ 0.00686726 -0.00760521 -0.06158827  0.01721958]
expert:[ 0.00686713 -0.00760532 -0.06158888  0.01721996]
expert:[ 0.006867   -0.00760544 -0.06158948  0.01722034]
expert:[ 0.00686687 -0.00760556 -0.06159008  0.01722071]
expert:[ 0.00686674 -0.00760568 -0.06159069  0.01722108]
expert:[ 0.00686662 -0.00760579 -0.06159128  0.01722146]
expert:[ 0.00686649 -0.00760591 -0.06159187  0.01722183]
expert:[ 0.00686637 -0.00760602 -0.06159247  0.01722219]
expert:[ 0.00686624 -0.00760614 -0.06159306  0.01722256]
expert:[ 0.00686612 -0.00760625 -0.06159365  0.01722293]
expert:[ 0.00686599 -0.00760637 -0.06159425  0.01722329]
expert:[ 0.00686587 -0.00760648 -0.06159484  0.01722366]
expert:[ 0.00686574 -0.0076066  -0.06159543  0.01722403]
expert:[ 0.00686561 -0.00760672 -0.06159603  0.0172244 ]
expert:[ 0.00686548 -0.00760683 -0.06159662  0.01722477]
expert:[ 0.00686537 -0.00760694 -0.06159717  0.01722513]
expert:[ 0.00686526 -0.00760704 -0.06159772  0.01722549]
expert:[ 0.00686516 -0.00760715 -0.06159827  0.01722585]
expert:[ 0.00686505 -0.00760726 -0.06159883  0.01722621]
expert:[ 0.00686495 -0.00760736 -0.0615994   0.01722658]
expert:[ 0.00686484 -0.00760747 -0.06159995  0.01722694]
expert:[ 0.00686474 -0.00760757 -0.06160051  0.0172273 ]
expert:[ 0.00686463 -0.00760768 -0.06160107  0.01722766]
expert:[ 0.00686453 -0.00760779 -0.06160162  0.01722802]
expert:[ 0.00686443 -0.00760789 -0.06160218  0.01722838]
expert:[ 0.00686432 -0.007608   -0.06160274  0.01722874]
expert:[ 0.00686422 -0.0076081  -0.0616033   0.01722911]
expert:[ 0.00686411 -0.00760821 -0.06160385  0.01722947]
expert:[ 0.00686401 -0.00760832 -0.06160441  0.01722983]
expert:[ 0.0068639  -0.00760842 -0.06160497  0.01723019]
expert:[ 0.0068638  -0.00760853 -0.06160552  0.01723055]
expert:[ 0.0068637  -0.00760864 -0.06160608  0.01723091]
expert:[ 0.00686359 -0.00760874 -0.06160664  0.01723127]
expert:[ 0.00686349 -0.00760885 -0.0616072   0.01723163]
expert:[ 0.00686338 -0.00760895 -0.06160776  0.017232  ]
expert:[ 0.00686328 -0.00760906 -0.06160831  0.01723236]
expert:[ 0.00686317 -0.00760917 -0.06160887  0.01723272]
expert:[ 0.00686307 -0.00760927 -0.06160943  0.01723308]
expert:[ 0.00686296 -0.00760938 -0.06160999  0.01723344]
expert:[ 0.00686286 -0.00760949 -0.06161055  0.0172338 ]
expert:[ 0.00686275 -0.00760959 -0.06161111  0.01723417]
expert:[ 0.00686265 -0.0076097  -0.06161167  0.01723453]
expert:[ 0.00686254 -0.00760981 -0.06161223  0.01723489]
expert:[ 0.00686244 -0.00760991 -0.06161278  0.01723525]
expert:[ 0.00686233 -0.00761002 -0.06161334  0.01723561]
expert:[ 0.00686223 -0.00761012 -0.0616139   0.01723597]
expert:[ 0.00686213 -0.00761023 -0.06161445  0.01723633]
expert:[ 0.00686204 -0.00761034 -0.06161499  0.01723666]
expert:[ 0.00686195 -0.00761045 -0.06161553  0.017237  ]
expert:[ 0.00686186 -0.00761055 -0.06161608  0.01723734]
expert:[ 0.00686177 -0.00761066 -0.06161662  0.01723767]
expert:[ 0.00686168 -0.00761077 -0.06161717  0.01723801]
expert:[ 0.00686159 -0.00761088 -0.06161771  0.01723835]
expert:[ 0.0068615  -0.00761099 -0.06161825  0.01723869]
expert:[ 0.00686141 -0.00761109 -0.06161879  0.01723903]
expert:[ 0.00686132 -0.0076112  -0.06161934  0.01723937]
expert:[ 0.00686123 -0.00761131 -0.06161988  0.0172397 ]
expert:[ 0.00686114 -0.00761142 -0.06162043  0.01724004]
expert:[ 0.00686105 -0.00761153 -0.06162097  0.01724038]
expert:[ 0.00686096 -0.00761163 -0.06162151  0.01724072]
expert:[ 0.00686087 -0.00761174 -0.06162205  0.01724106]
expert:[ 0.00686078 -0.00761185 -0.0616226   0.01724139]
expert:[ 0.00686068 -0.00761196 -0.06162315  0.01724175]
expert:[ 0.00686059 -0.00761207 -0.06162372  0.0172421 ]
expert:[ 0.00686049 -0.00761218 -0.06162427  0.01724244]
expert:[ 0.00686039 -0.00761229 -0.06162484  0.01724279]
expert:[ 0.0068603  -0.0076124  -0.0616254   0.01724315]
expert:[ 0.0068602  -0.00761251 -0.06162596  0.0172435 ]
expert:[ 0.00686011 -0.00761262 -0.06162652  0.01724385]
expert:[ 0.00686001 -0.00761274 -0.06162708  0.01724419]
expert:[ 0.00685991 -0.00761285 -0.06162764  0.01724455]
expert:[ 0.00685982 -0.00761296 -0.0616282   0.01724489]
expert:[ 0.00685972 -0.00761307 -0.06162876  0.01724524]
expert:[ 0.00685962 -0.00761318 -0.06162932  0.01724559]
expert:[ 0.00685953 -0.00761329 -0.06162987  0.01724594]
expert:[ 0.00685943 -0.0076134  -0.06163044  0.01724629]
expert:[ 0.00685933 -0.00761351 -0.06163099  0.01724665]
expert:[ 0.00685924 -0.00761362 -0.06163157  0.017247  ]
expert:[ 0.00685916 -0.00761375 -0.06163214  0.01724735]
expert:[ 0.00685908 -0.00761389 -0.06163272  0.0172477 ]
expert:[ 0.006859   -0.00761402 -0.06163331  0.01724806]
expert:[ 0.00685892 -0.00761415 -0.06163389  0.01724841]
expert:[ 0.00685884 -0.00761429 -0.06163447  0.01724876]
expert:[ 0.00685876 -0.00761442 -0.06163505  0.01724911]
expert:[ 0.00685868 -0.00761455 -0.06163564  0.01724947]
expert:[ 0.0068586  -0.00761469 -0.06163622  0.01724982]
expert:[ 0.00685852 -0.00761482 -0.0616368   0.01725017]
expert:[ 0.00685844 -0.00761495 -0.06163738  0.01725053]
expert:[ 0.00685836 -0.00761509 -0.06163796  0.01725088]
expert:[ 0.00685829 -0.00761522 -0.06163855  0.01725123]
expert:[ 0.00685821 -0.00761535 -0.06163913  0.01725159]
expert:[ 0.00685813 -0.00761549 -0.06163971  0.01725194]
expert:[ 0.00685805 -0.00761562 -0.06164029  0.0172523 ]
expert:[ 0.00685797 -0.00761575 -0.06164087  0.01725265]
expert:[ 0.00685789 -0.00761589 -0.06164147  0.017253  ]
expert:[ 0.00685782 -0.00761603 -0.06164207  0.01725336]
expert:[ 0.00685774 -0.00761617 -0.06164271  0.01725376]
expert:[ 0.00685766 -0.00761632 -0.06164336  0.01725415]
expert:[ 0.00685759 -0.00761645 -0.061644    0.01725454]
expert:[ 0.00685753 -0.00761658 -0.06164463  0.0172549 ]
expert:[ 0.00685747 -0.0076167  -0.06164527  0.01725527]
expert:[ 0.00685741 -0.00761683 -0.0616459   0.01725564]
expert:[ 0.00685736 -0.00761695 -0.06164654  0.017256  ]
expert:[ 0.00685731 -0.00761708 -0.06164718  0.01725637]
expert:[ 0.00685725 -0.00761721 -0.06164782  0.01725674]
expert:[ 0.0068572  -0.00761734 -0.06164846  0.01725711]
expert:[ 0.00685715 -0.00761746 -0.0616491   0.01725748]
expert:[ 0.00685709 -0.00761759 -0.06164974  0.01725784]
expert:[ 0.00685704 -0.00761772 -0.06165038  0.01725821]
expert:[ 0.00685699 -0.00761785 -0.06165102  0.01725858]
expert:[ 0.00685694 -0.00761797 -0.06165166  0.01725895]
expert:[ 0.00685688 -0.0076181  -0.0616523   0.01725931]
expert:[ 0.00685683 -0.00761823 -0.06165294  0.01725968]
expert:[ 0.00685677 -0.00761835 -0.06165358  0.01726005]
expert:[ 0.00685672 -0.00761848 -0.06165422  0.01726042]
expert:[ 0.00685667 -0.00761861 -0.06165486  0.01726079]
expert:[ 0.00685662 -0.00761874 -0.06165549  0.01726115]
expert:[ 0.00685656 -0.00761886 -0.06165613  0.01726152]
expert:[ 0.00685651 -0.00761899 -0.06165678  0.01726189]
expert:[ 0.00685646 -0.00761912 -0.06165741  0.01726226]
expert:[ 0.0068564  -0.00761925 -0.06165805  0.01726263]
expert:[ 0.00685635 -0.00761937 -0.06165869  0.01726299]
expert:[ 0.0068563  -0.0076195  -0.06165933  0.01726336]
expert:[ 0.00685624 -0.00761963 -0.06165997  0.01726373]
expert:[ 0.00685619 -0.00761976 -0.06166061  0.0172641 ]
expert:[ 0.00685614 -0.00761988 -0.06166125  0.01726446]
expert:[ 0.00685608 -0.00762001 -0.06166189  0.01726483]
expert:[ 0.00685603 -0.00762014 -0.06166253  0.0172652 ]
expert:[ 0.00685597 -0.00762027 -0.06166317  0.01726557]
expert:[ 0.00685592 -0.00762039 -0.06166381  0.01726594]
expert:[ 0.00685587 -0.00762052 -0.06166445  0.01726631]
expert:[ 0.00685581 -0.00762065 -0.06166509  0.01726667]
expert:[ 0.00685576 -0.00762077 -0.06166573  0.01726704]
expert:[ 0.00685571 -0.0076209  -0.06166637  0.01726741]
expert:[ 0.00685566 -0.00762103 -0.06166701  0.01726777]
expert:[ 0.00685561 -0.00762116 -0.06166765  0.01726814]
expert:[ 0.00685556 -0.00762128 -0.06166828  0.01726851]
expert:[ 0.00685551 -0.00762141 -0.06166893  0.01726887]
expert:[ 0.00685547 -0.00762153 -0.06166957  0.01726924]
expert:[ 0.00685542 -0.00762166 -0.06167021  0.0172696 ]
expert:[ 0.00685537 -0.00762178 -0.06167085  0.01726997]
expert:[ 0.00685532 -0.00762191 -0.06167149  0.01727033]
expert:[ 0.00685528 -0.00762203 -0.06167213  0.0172707 ]
expert:[ 0.00685523 -0.00762216 -0.06167277  0.01727106]
expert:[ 0.00685518 -0.00762229 -0.06167341  0.01727143]
expert:[ 0.00685513 -0.00762241 -0.06167405  0.01727179]
expert:[ 0.00685509 -0.00762254 -0.06167469  0.01727216]
expert:[ 0.00685504 -0.00762266 -0.06167533  0.01727252]
expert:[ 0.00685499 -0.00762279 -0.06167597  0.01727289]
expert:[ 0.00685494 -0.00762291 -0.06167661  0.01727326]
expert:[ 0.0068549  -0.00762304 -0.06167726  0.01727362]
expert:[ 0.00685485 -0.00762317 -0.06167789  0.01727398]
expert:[ 0.0068548  -0.00762329 -0.06167854  0.01727435]
expert:[ 0.00685476 -0.00762342 -0.06167914  0.0172747 ]
expert:[ 0.0068547  -0.00762355 -0.06167971  0.01727504]
expert:[ 0.00685465 -0.00762369 -0.06168027  0.01727539]
expert:[ 0.00685459 -0.00762382 -0.06168083  0.01727574]
expert:[ 0.00685454 -0.00762395 -0.06168139  0.01727609]
expert:[ 0.00685448 -0.00762409 -0.06168196  0.01727644]
expert:[ 0.00685442 -0.00762422 -0.06168253  0.01727678]
expert:[ 0.00685437 -0.00762435 -0.06168308  0.01727713]
expert:[ 0.00685431 -0.00762449 -0.06168365  0.01727748]
expert:[ 0.00685426 -0.00762463 -0.06168424  0.01727783]
expert:[ 0.0068542  -0.00762476 -0.06168482  0.0172782 ]
expert:[ 0.00685415 -0.0076249  -0.0616854   0.01727856]
expert:[ 0.00685409 -0.00762504 -0.06168599  0.01727892]
expert:[ 0.00685404 -0.00762518 -0.06168658  0.01727928]
expert:[ 0.00685398 -0.00762532 -0.06168716  0.01727965]
expert:[ 0.00685393 -0.00762546 -0.06168775  0.01728001]
expert:[ 0.00685387 -0.00762559 -0.06168834  0.01728037]
expert:[ 0.00685382 -0.00762573 -0.06168893  0.01728073]
expert:[ 0.00685376 -0.00762586 -0.06168951  0.01728109]
expert:[ 0.00685371 -0.00762582 -0.06169016  0.01728152]
expert:[ 0.00685366 -0.00762578 -0.0616908   0.01728195]
expert:[ 0.00685361 -0.00762574 -0.06169144  0.01728237]
expert:[ 0.00685356 -0.0076257  -0.06169209  0.0172828 ]
expert:[ 0.0068535  -0.00762566 -0.06169273  0.01728323]
expert_success: 0
path_length: 200



-------------------------------- Iteration 0 -------------------------------- 
initial ob: [-0.03265025  0.5148823   0.23687374  0.          0.6         0.02
  0.24        0.74        0.11        0.24        0.74        0.11      ]
agent_success: 0

training time:  0.1852121353149414
evaluation time:  10.895704746246338
epoch time:  11.080944538116455


-------------------------------- Training stopped due to early stopping -------------------------------- 
min loss:  0.0043370975


-------------------------------- Test Results -------------------------------- 
initial ob: [-0.03265025  0.5148823   0.23687374  0.          0.6         0.02
  0.24        0.74        0.11        0.24        0.74        0.11      ]
agent_success: 0

mean_success_rate:  0.0
Done
