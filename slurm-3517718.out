SLURM_JOBID=3517718
working directory=/viscam/u/jialugao/Imitation-SoftModule
{'obs_type': 'with_goal'}
obs type:  with_goal
{'expert_policy_file': 'policy/expert/MT10_Fixed/', 'exp_name': 'mt10_fixed', 'do_dagger': False, 'ep_len': 200, 'gradient_steps': 1, 'n_iter': 100, 'render_interval': 100, 'eval_interval': 1, 'batch_size': 64, 'eval_batch_size': 32, 'train_batch_size': 32, 'n_layers': 2, 'size': 400, 'learning_rate': 0.0001, 'video_log_freq': -1, 'scalar_log_freq': 1, 'no_gpu': False, 'which_gpu': 0, 'max_replay_buffer_size': 1000000, 'save_params': False, 'seed': 32, 'worker_nums': 1, 'eval_worker_nums': 1, 'config': 'config/soft_module_fixed_mt10.json', 'no_cuda': True, 'random_init': False, 'device': 'cpu', 'id': 'MT10_Single_Task', 'task_name': None, 'task_env': 'MT10_task_env', 'cuda': False, 'agent_class': <class 'agents.bc_agent.SoftModuleAgent'>, 'agent_params': {'n_layers': 2, 'size': 400, 'learning_rate': 0.0001, 'max_replay_buffer_size': 1000000, 'discrete': False}}
{'env_name': 'mt10', 'env': {'reward_scale': 1, 'obs_norm': False}, 'meta_env': {'obs_type': 'with_goal', 'random_init': False}, 'replay_buffer': {'size': 1000000.0}, 'expert_net': {'hidden_shapes': [400, 400], 'append_hidden_shapes': [400]}, 'net': {'hidden_shapes': [400, 400], 'em_hidden_shapes': [400], 'num_layers': 2, 'num_modules': 2, 'module_hidden': 256, 'num_gating_layers': 2, 'gating_hidden': 256, 'add_bn': False, 'pre_softmax': False, 'base_type': <class 'networks.base.MLPBase'>}, 'general_setting': {'discount': 0.99, 'pretrain_epochs': 20, 'num_epochs': 7500, 'epoch_frames': 200, 'max_episode_frames': 200, 'batch_size': 1280, 'min_pool': 10000, 'target_hard_update_period': 1000, 'use_soft_update': True, 'tau': 0.005, 'opt_times': 200, 'eval_episodes': 1, 'train_render': False, 'eval_render': True, 'env': <NormAct<RewardShift<MTEnv instance>>>, 'device': device(type='cpu')}, 'sac': {'plr': 0.0003, 'qlr': 0.0003, 'reparameterization': True, 'automatic_entropy_tuning': True, 'policy_std_reg_weight': 0, 'policy_mean_reg_weight': 0}}
Loading expert policy from... policy/expert/MT10_Fixed/
Done restoring expert policy...
<NormAct<RewardShift<AugObs<SawyerReachPushPickPlaceEnv instance>>>>
<NormAct<RewardShift<AugObs<SawyerReachPushPickPlaceEnv instance>>>>
<NormAct<RewardShift<AugObs<SawyerReachPushPickPlaceEnv instance>>>>
<NormAct<RewardShift<AugObs<SawyerDoorEnv instance>>>>
<NormAct<RewardShift<AugObs<SawyerDrawerOpenEnv instance>>>>
<NormAct<RewardShift<AugObs<SawyerDrawerCloseEnv instance>>>>
<NormAct<RewardShift<AugObs<SawyerButtonPressTopdownEnv instance>>>>
<NormAct<RewardShift<AugObs<SawyerPegInsertionSideEnv instance>>>>
<NormAct<RewardShift<AugObs<SawyerWindowOpenEnv instance>>>>
<NormAct<RewardShift<AugObs<SawyerWindowCloseEnv instance>>>>
/viscam/u/jialugao/.anaconda3/envs/metaworldEnv/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Could not seed environment <MTEnv instance>[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
<string>:6: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.


-------------------------------- Iteration 0 -------------------------------- 
initial ob: [-0.03265039  0.51487777  0.2368754   0.          0.6         0.01492813
 -0.1         0.8         0.2        -0.1         0.8         0.2       ]
expert_success: 1.0
path_length: 150

initial ob: [-0.03265039  0.51487777  0.2368754   0.          0.6         0.01492813
  0.1         0.8         0.02        0.1         0.8         0.02      ]
expert_success: 1.0
path_length: 150

initial ob: [-0.03265039  0.51487777  0.2368754   0.          0.6         0.01492813
  0.1         0.8         0.2         0.1         0.8         0.2       ]
expert_success: 0
path_length: 150

initial ob: [-0.03264849  0.51488124  0.2368859   0.2         0.79999999  0.15
 -0.2         0.7         0.15       -0.2         0.7         0.15      ]
expert_success: 0
path_length: 150

initial ob: [-0.03445723  0.50704204  0.23981373  0.          0.69999998  0.09
  0.          0.55        0.04        0.          0.55        0.04      ]
expert_success: 0
path_length: 150

initial ob: [-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.7         0.04      ]
expert_success: 1.0
path_length: 150

initial ob: [-0.03265259  0.51487763  0.23688885  0.          0.88        0.13
  0.          0.88000001  0.1         0.          0.88000001  0.1       ]
expert_success: 1.0
path_length: 150

initial ob: [-0.03265225  0.51488206  0.23688323  0.          0.6         0.02
 -0.2         0.6         0.13       -0.2         0.6         0.13      ]
expert_success: 0
path_length: 150

initial ob: [-0.03445341  0.50704491  0.23981405 -0.12        0.73000003  0.15000001
  0.08        0.785       0.15        0.08        0.785       0.15      ]
expert_success: 0
path_length: 150

initial ob: [-0.03445605  0.50704462  0.23981209  0.12        0.73000003  0.15000001
 -0.08        0.785       0.15       -0.08        0.785       0.15      ]
expert_success: 0
path_length: 150

{'reach-v1_success_rate': array(1., dtype=float32), 'push-v1_success_rate': array(1., dtype=float32), 'pick-place-v1_success_rate': array(0., dtype=float32), 'door-v1_success_rate': array(0., dtype=float32), 'drawer-open-v1_success_rate': array(0., dtype=float32), 'drawer-close-v1_success_rate': array(1., dtype=float32), 'button-press-topdown-v1_success_rate': array(1., dtype=float32), 'ped-insert-side-v1_success_rate': array(0., dtype=float32), 'window-open-v1_success_rate': array(0., dtype=float32), 'window-close-v1_success_rate': array(0., dtype=float32), 'mean_success_rate': 0.4}


-------------------------------- Iteration 0 -------------------------------- 
Found 0 GPUs for rendering. Using device 0.
Device id outside of range of available devices.
Traceback (most recent call last):
  File "train_soft_module.py", line 277, in <module>
    main()
  File "train_soft_module.py", line 274, in main
    trainer.run_training_loop()
  File "train_soft_module.py", line 188, in run_training_loop
    agent_task_curve=self.agent_task_curve
  File "/viscam/u/jialugao/Imitation-SoftModule/torch_rl/rl_trainer.py", line 286, in run_training_loop
    eval_infos = self.agent_env.sample_agent(log_prefix=self.plot_prefix, agent_policy=self.agent.actor.policy, input_shape = self.agent.actor.input_shape, render=render)
  File "/viscam/u/jialugao/Imitation-SoftModule/torch_rl/multi_task_collector.py", line 358, in sample_agent
    self.build_Multi_task_env(agent_policy=agent_policy, input_shape=input_shape, render=render)
  File "/viscam/u/jialugao/Imitation-SoftModule/torch_rl/multi_task_collector.py", line 450, in build_Multi_task_env
    input_shape=input_shape)
  File "/viscam/u/jialugao/Imitation-SoftModule/torch_rl/multi_task_collector.py", line 538, in evaluate
    image = env_info.env.get_image(400,400,"leftview")
  File "/viscam/u/jialugao/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 170, in get_image
    camera_name=camera_name,
  File "mujoco_py/mjsim.pyx", line 156, in mujoco_py.cymj.MjSim.render
  File "mujoco_py/mjsim.pyx", line 158, in mujoco_py.cymj.MjSim.render
  File "mujoco_py/mjrendercontext.pyx", line 46, in mujoco_py.cymj.MjRenderContext.__init__
  File "mujoco_py/mjrendercontext.pyx", line 114, in mujoco_py.cymj.MjRenderContext._setup_opengl_context
  File "mujoco_py/opengl_context.pyx", line 130, in mujoco_py.cymj.OffscreenOpenGLContext.__init__
RuntimeError: Failed to initialize OpenGL
srun: error: viscam2: task 0: Exited with exit code 1
Done
