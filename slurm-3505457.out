SLURM_JOBID=3505457
working directory=/viscam/u/jialugao/Imitation-SoftModule
{'obs_type': 'with_goal'}
{'expert_policy_file': 'policy/expert/MT50_Fixed/', 'exp_name': 'mt50_fixed', 'do_dagger': False, 'ep_len': 200, 'num_agent_train_steps_per_iter': 1000, 'n_iter': 500, 'render_interval': 500, 'batch_size': 64, 'eval_batch_size': 32, 'train_batch_size': 32, 'n_layers': 2, 'size': 400, 'learning_rate': 0.0001, 'video_log_freq': -1, 'scalar_log_freq': 1, 'no_gpu': False, 'which_gpu': 0, 'max_replay_buffer_size': 1000000, 'save_params': False, 'seed': 32, 'worker_nums': 1, 'eval_worker_nums': 1, 'config': 'config/soft_module_fixed_mt50.json', 'no_cuda': True, 'random_init': False, 'device': 'cpu', 'id': 'MT50_Single_Task', 'task_name': None, 'task_env': 'MT50_task_env', 'cuda': False, 'agent_class': <class 'agents.bc_agent.SoftModuleAgent'>, 'agent_params': {'n_layers': 2, 'size': 400, 'learning_rate': 0.0001, 'max_replay_buffer_size': 1000000, 'discrete': False, 'ac_dim': 4, 'ob_dim': 12}}
{'env_name': 'mt50', 'env': {'reward_scale': 1, 'obs_norm': False}, 'meta_env': {'obs_type': 'with_goal', 'random_init': False}, 'replay_buffer': {'size': 1000000.0}, 'expert_net': {'hidden_shapes': [400, 400], 'append_hidden_shapes': [400]}, 'net': {'hidden_shapes': [400, 400], 'em_hidden_shapes': [400], 'num_layers': 2, 'num_modules': 2, 'module_hidden': 256, 'num_gating_layers': 2, 'gating_hidden': 256, 'add_bn': False, 'pre_softmax': False, 'base_type': <class 'networks.base.MLPBase'>}, 'general_setting': {'discount': 0.99, 'pretrain_epochs': 20, 'num_epochs': 7500, 'epoch_frames': 200, 'max_episode_frames': 200, 'batch_size': 1280, 'min_pool': 10000, 'target_hard_update_period': 1000, 'use_soft_update': True, 'tau': 0.005, 'opt_times': 200, 'eval_episodes': 1, 'train_render': False, 'eval_render': True, 'env': <NormAct<RewardShift<MTEnv instance>>>, 'device': device(type='cpu')}, 'sac': {'plr': 0.0003, 'qlr': 0.0003, 'reparameterization': True, 'automatic_entropy_tuning': True, 'policy_std_reg_weight': 0, 'policy_mean_reg_weight': 0}}
Loading expert policy from... policy/expert/MT50_Fixed/
Done restoring expert policy...
<NormAct<RewardShift<SawyerReachPushPickPlaceEnv instance>>>
<NormAct<RewardShift<SawyerReachPushPickPlaceEnv instance>>>
<NormAct<RewardShift<SawyerReachPushPickPlaceEnv instance>>>
<NormAct<RewardShift<SawyerReachPushPickPlaceWallEnv instance>>>
<NormAct<RewardShift<SawyerReachPushPickPlaceWallEnv instance>>>
<NormAct<RewardShift<SawyerReachPushPickPlaceWallEnv instance>>>
<NormAct<RewardShift<SawyerDoorEnv instance>>>
<NormAct<RewardShift<SawyerDoorCloseEnv instance>>>
<NormAct<RewardShift<SawyerDrawerOpenEnv instance>>>
<NormAct<RewardShift<SawyerDrawerCloseEnv instance>>>
<NormAct<RewardShift<SawyerButtonPressTopdownEnv instance>>>
<NormAct<RewardShift<SawyerButtonPressEnv instance>>>
<NormAct<RewardShift<SawyerButtonPressTopdownWallEnv instance>>>
<NormAct<RewardShift<SawyerButtonPressWallEnv instance>>>
<NormAct<RewardShift<SawyerPegInsertionSideEnv instance>>>
<NormAct<RewardShift<SawyerPegUnplugSideEnv instance>>>
<NormAct<RewardShift<SawyerWindowOpenEnv instance>>>
<NormAct<RewardShift<SawyerWindowCloseEnv instance>>>
<NormAct<RewardShift<SawyerNutDisassembleEnv instance>>>
<NormAct<RewardShift<SawyerHammerEnv instance>>>
<NormAct<RewardShift<SawyerPlateSlideEnv instance>>>
<NormAct<RewardShift<SawyerPlateSlideSideEnv instance>>>
<NormAct<RewardShift<SawyerPlateSlideBackEnv instance>>>
<NormAct<RewardShift<SawyerPlateSlideBackSideEnv instance>>>
<NormAct<RewardShift<SawyerHandlePressEnv instance>>>
<NormAct<RewardShift<SawyerHandlePullEnv instance>>>
<NormAct<RewardShift<SawyerHandlePressSideEnv instance>>>
<NormAct<RewardShift<SawyerHandlePullSideEnv instance>>>
<NormAct<RewardShift<SawyerStickPushEnv instance>>>
<NormAct<RewardShift<SawyerStickPullEnv instance>>>
<NormAct<RewardShift<SawyerBasketballEnv instance>>>
<NormAct<RewardShift<SawyerSoccerEnv instance>>>
<NormAct<RewardShift<SawyerFaucetOpenEnv instance>>>
<NormAct<RewardShift<SawyerFaucetCloseEnv instance>>>
<NormAct<RewardShift<SawyerCoffeePushEnv instance>>>
<NormAct<RewardShift<SawyerCoffeePullEnv instance>>>
<NormAct<RewardShift<SawyerCoffeeButtonEnv instance>>>
<NormAct<RewardShift<SawyerSweepEnv instance>>>
<NormAct<RewardShift<SawyerSweepIntoGoalEnv instance>>>
<NormAct<RewardShift<SawyerPickOutOfHoleEnv instance>>>
<NormAct<RewardShift<SawyerNutAssemblyEnv instance>>>
<NormAct<RewardShift<SawyerShelfPlaceEnv instance>>>
<NormAct<RewardShift<SawyerPushBackEnv instance>>>
<NormAct<RewardShift<SawyerLeverPullEnv instance>>>
<NormAct<RewardShift<SawyerDialTurnEnv instance>>>
<NormAct<RewardShift<SawyerBinPickingEnv instance>>>
<NormAct<RewardShift<SawyerBoxCloseEnv instance>>>
<NormAct<RewardShift<SawyerHandInsertEnv instance>>>
<NormAct<RewardShift<SawyerDoorLockEnv instance>>>
<NormAct<RewardShift<SawyerDoorUnlockEnv instance>>>
/viscam/u/jialugao/.anaconda3/envs/metaworldEnv/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Could not seed environment <MTEnv instance>[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))


-------------------------------- Iteration 0 -------------------------------- 
{'reach-v1_success_rate': array(1., dtype=float32), 'push-v1_success_rate': array(0., dtype=float32), 'pick-place-v1_success_rate': array(0., dtype=float32), 'reach-wall-v1_success_rate': array(1., dtype=float32), 'pick-place-wall-v1_success_rate': array(0., dtype=float32), 'push-wall-v1_success_rate': array(0., dtype=float32), 'door-open-v1_success_rate': array(0., dtype=float32), 'door-close-v1_success_rate': array(0., dtype=float32), 'drawer-open-v1_success_rate': array(0., dtype=float32), 'drawer-close-v1_success_rate': array(0., dtype=float32), 'button-press_topdown-v1_success_rate': array(0., dtype=float32), 'button-press-v1_success_rate': array(1., dtype=float32), 'button-press-topdown-wall-v1_success_rate': array(1., dtype=float32), 'button-press-wall-v1_success_rate': array(1., dtype=float32), 'peg-insert-side-v1_success_rate': array(0., dtype=float32), 'peg-unplug-side-v1_success_rate': array(0., dtype=float32), 'window-open-v1_success_rate': array(1., dtype=float32), 'window-close-v1_success_rate': array(0., dtype=float32), 'dissassemble-v1_success_rate': array(0., dtype=float32), 'hammer-v1_success_rate': array(0., dtype=float32), 'plate-slide-v1_success_rate': array(1., dtype=float32), 'plate-slide-side-v1_success_rate': array(0., dtype=float32), 'plate-slide-back-v1_success_rate': array(0., dtype=float32), 'plate-slide-back-side-v1_success_rate': array(0., dtype=float32), 'handle-press-v1_success_rate': array(1., dtype=float32), 'handle-pull-v1_success_rate': array(1., dtype=float32), 'handle-press-side-v1_success_rate': array(0., dtype=float32), 'handle-pull-side-v1_success_rate': array(0., dtype=float32), 'stick-push-v1_success_rate': array(0., dtype=float32), 'stick-pull-v1_success_rate': array(0., dtype=float32), 'basket-ball-v1_success_rate': array(0., dtype=float32), 'soccer-v1_success_rate': array(1., dtype=float32), 'faucet-open-v1_success_rate': array(0., dtype=float32), 'faucet-close-v1_success_rate': array(0., dtype=float32), 'coffee-push-v1_success_rate': array(1., dtype=float32), 'coffee-pull-v1_success_rate': array(0., dtype=float32), 'coffee-button-v1_success_rate': array(1., dtype=float32), 'sweep-v1_success_rate': array(1., dtype=float32), 'sweep-into-v1_success_rate': array(1., dtype=float32), 'pick-out-of-hole-v1_success_rate': array(0., dtype=float32), 'assembly-v1_success_rate': array(0., dtype=float32), 'shelf-place-v1_success_rate': array(1., dtype=float32), 'push-back-v1_success_rate': array(0., dtype=float32), 'lever-pull-v1_success_rate': array(0., dtype=float32), 'dial-turn-v1_success_rate': array(1., dtype=float32), 'bin-picking-v1_success_rate': array(0., dtype=float32), 'box-close-v1_success_rate': array(0., dtype=float32), 'hand-insert-v1_success_rate': array(1., dtype=float32), 'door-lock-v1_success_rate': array(1., dtype=float32), 'door-unlock-v1_success_rate': array(1., dtype=float32), 'mean_success_rate': 0.38}
Traceback (most recent call last):
  File "train_soft_module.py", line 232, in <module>
    main()
  File "train_soft_module.py", line 229, in main
    trainer.run_training_loop()
  File "train_soft_module.py", line 144, in run_training_loop
    agent_task_curve=self.agent_task_curve
  File "/viscam/u/jialugao/Imitation-SoftModule/torch_rl/rl_trainer.py", line 187, in run_training_loop
    self.agent.add_to_replay_buffer(paths)
  File "/viscam/u/jialugao/Imitation-SoftModule/agents/bc_agent.py", line 109, in add_to_replay_buffer
    self.replay_buffer.add_mt_rollouts(paths)
  File "/viscam/u/jialugao/Imitation-SoftModule/torch_rl/replay_buffer.py", line 104, in add_mt_rollouts
    convert_listofrollouts(paths, concat_rew))
  File "/viscam/u/jialugao/Imitation-SoftModule/utils/utils.py", line 33, in convert_listofrollouts
    observations = np.concatenate([path["observation"] for path in paths])
  File "<__array_function__ internals>", line 6, in concatenate
ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 9 and the array at index 28 has size 12
srun: error: viscam1: task 0: Exited with exit code 1
Done
